# robots-txt-ai-analyzer

https://www.robotstxt.org/

## What is
Web Robots (also known as Web Wanderers, Crawlers, or Spiders), are programs that traverse the Web automatically. Search engines use them to index the web content, spammers use them to scan for email addresses, and they have many other uses.

Web site owners use the /robots.txt file to give instructions about their site to web robots; this is called The Robots Exclusion Protocol.

The "User-agent: *" means this section applies to all robots. The "Disallow: /" tells the robot that it should not visit any pages on the site.

https://www.robotstxt.org/orig.html

https://www.robotstxt.org/norobots-rfc.txt

## RULES:
https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt#create_rules

